ğŸ¶ Speech Emotion Recognition

This project is an implementation of Speech Emotion Recognition (SER) using deep learning and MFCC feature extraction.
It predicts emotions such as Angry, Disgust, Fear, Happy, Neutral, Sad, and Surprise from audio recordings.

Dataset used: ğŸ¤ Toronto Emotional Speech Set (TESS)

ğŸŒŸ Highlights

âœ”ï¸ Recognizes 7 emotions from speech
âœ”ï¸ Uses MFCC features for robust audio representation
âœ”ï¸ Built with Deep Learning (LSTM/Conv1D)
âœ”ï¸ Works on real-time input audio files
âœ”ï¸ Provides confidence score for predictions
âœ”ï¸ Includes training visualizations (accuracy & loss curves)

ğŸ“‚ Dataset Overview

Name: TESS (Toronto Emotional Speech Set)

Samples: ~5600 audio clips

Format: .wav

Speakers: 2 female actors (ages 26 and 64)

Emotions: Angry, Disgust, Fear, Happy, Neutral, Sad, Surprise

ğŸ“Š Results & Visualizations

Training Accuracy: ~95%

Testing Accuracy: ~90%

ğŸ“ˆ Example training graph:

ğŸš€ How It Works

Feature Extraction â†’ Extracts MFCCs from audio signals.

Model Training â†’ Trains a deep learning model (LSTM/Conv1D).

Prediction â†’ Identifies the emotion from new speech samples with confidence scores.

ğŸ”® Future Scope

Extend support for real-world noisy audio

Explore Bidirectional LSTM / Attention-based models

Integrate with RAVDESS, CREMA-D, Emo-DB datasets

Build a real-time emotion recognition app
